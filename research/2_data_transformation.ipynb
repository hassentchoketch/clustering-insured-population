{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b56ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6b413a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\tchok\\\\OneDrive\\\\Bureau\\\\My_github\\\\clustering-insured-population\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95cf5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\tchok\\\\OneDrive\\\\Bureau\\\\My_github\\\\clustering-insured-population'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f7cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    transformed_data_path: Path\n",
    "    transforming_data_path: Path\n",
    "    transformer_path : Path \n",
    "    metric_features : list\n",
    "    non_metric_features : list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d53cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from insuredSegmenter.constants import * \n",
    "from insuredSegmenter.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca4965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_path: Path = CONFIG_FILE_PATH, \n",
    "        params_path: Path = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(str(config_path))\n",
    "        self.params = read_yaml(str(params_path))\n",
    "        \n",
    "        create_directories([self.config.artifacts_root]) # create directories if they do not exist\n",
    "\n",
    "    def get_transformation_config(self) -> DataTransformationConfig:\n",
    "        transforming_data_path = self.config.data_ingestion.csv\n",
    "        transformation = self.config.data_transformation\n",
    "        transformed_data_path = transformation.transformed_data_path\n",
    "        transforer_path = transformation.transformer_path\n",
    "        metric_features = transformation.metric_features\n",
    "        non_metric_features = transformation.non_metric_features\n",
    "        \n",
    "        \n",
    "        create_directories([transformation.root_dir])\n",
    "    \n",
    "        # create_directories([transformation.root_dir])\n",
    "        transformation_config = DataTransformationConfig(\n",
    "            root_dir= Path(transformation.root_dir),\n",
    "            transforming_data_path= Path(transforming_data_path),\n",
    "            transformed_data_path= Path(transformed_data_path),\n",
    "            transformer_path= Path(transforer_path),\n",
    "            metric_features= metric_features,\n",
    "            non_metric_features= non_metric_features\n",
    "            \n",
    "        )\n",
    "        return transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33efce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to remove outliers using Z-score method.\n",
    "    This implements scikit-learn's transformer interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_threshold=3.0):\n",
    "        self.z_threshold = z_threshold\n",
    "        self.feature_indices_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Store indices of features with outliers to be used in transform\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_transformed = X.copy()\n",
    "            # Apply z-score thresholding to each column\n",
    "            for col in X_transformed.columns:\n",
    "                if X_transformed[col].dtype in [np.float64, np.int64]:\n",
    "                    z_scores = np.abs(stats.zscore(X_transformed[col], nan_policy='omit'))\n",
    "                    X_transformed.loc[z_scores >= self.z_threshold, col] = np.nan\n",
    "            return X_transformed\n",
    "        else:\n",
    "            # If not DataFrame, convert to numpy array\n",
    "            X_transformed = np.copy(X)\n",
    "            # Apply z-score thresholding to each column\n",
    "            for col in range(X_transformed.shape[1]):\n",
    "                z_scores = np.abs(stats.zscore(X_transformed[:, col], nan_policy='omit'))\n",
    "                X_transformed[z_scores >= self.z_threshold, col] = np.nan\n",
    "            return X_transformed\n",
    "\n",
    "class SkewnessTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to handle skewed data automatically.\n",
    "    Applies log transformation to highly skewed features.\n",
    "    \"\"\"\n",
    "    def __init__(self, skew_threshold=1.0):\n",
    "        self.skew_threshold = skew_threshold\n",
    "        self.skewed_features_ = {}  # Will store skewed features and their min values\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Identify skewed features\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in X.columns:\n",
    "                if X[col].dtype in [np.float64, np.int64]:\n",
    "                    skewness = X[col].skew()\n",
    "                    if abs(skewness) > self.skew_threshold:\n",
    "                        # Store min value to use in transformation\n",
    "                        min_val = X[col].min()\n",
    "                        self.skewed_features_[col] = min_val\n",
    "        else:\n",
    "            # For numpy arrays, we'll transform all numeric columns\n",
    "            for col in range(X.shape[1]):\n",
    "                if np.issubdtype(X[:, col].dtype, np.number):\n",
    "                    # Use pandas Series for skew calculation\n",
    "                    skewness = pd.Series(X[:, col]).skew()\n",
    "                    if abs(skewness) > self.skew_threshold:\n",
    "                        min_val = np.min(X[:, col])\n",
    "                        self.skewed_features_[col] = min_val\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy() if isinstance(X, pd.DataFrame) else np.copy(X)\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col, min_val in self.skewed_features_.items():\n",
    "                if col in X_transformed.columns:\n",
    "                    # Apply log transformation (adding small constant to handle zeros)\n",
    "                    X_transformed[col] = np.log1p(X_transformed[col] - min_val + 0.01)\n",
    "        else:\n",
    "            for col, min_val in self.skewed_features_.items():\n",
    "                # Apply log transformation to numpy array\n",
    "                X_transformed[:, col] = np.log1p(X_transformed[:, col] - min_val + 0.01)\n",
    "                \n",
    "        return X_transformed\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def _save_object(self, obj, file_path: Path):\n",
    "        \"\"\"\n",
    "        This function is used to save the object to the specified path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(obj, file)\n",
    "        except Exception as e:\n",
    "            raise (e)\n",
    "    \n",
    "    def get_data_transformer_object(self, remove_outliers: bool = True) -> Pipeline:\n",
    "        \"\"\"\n",
    "        This function is used to transform the data using the following steps:\n",
    "        1. remove outliers from numerical columns (optional)\n",
    "        2. handle skewed numerical features\n",
    "        3. impute missing values with median for numerical columns\n",
    "        4. transform numerical data using PowerTransformer\n",
    "        5. impute missing values with most frequent value for categorical columns\n",
    "        6. one-hot encode categorical data\n",
    "        \n",
    "        The entire transformation is wrapped in a single Pipeline for easy saving and reuse.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define the preprocessor steps\n",
    "            preprocessing_steps = []\n",
    "            \n",
    "            # Step 1: Add outlier removal if requested\n",
    "            if remove_outliers:\n",
    "                preprocessing_steps.append((\"outlier_remover\", OutlierRemover(z_threshold=3.0)))\n",
    "                \n",
    "            # Step 2: Add skewness transformer\n",
    "            preprocessing_steps.append((\"skewness_transformer\", SkewnessTransformer(skew_threshold=1.0)))\n",
    "            \n",
    "            # Step 3: Column transformer for different column types\n",
    "            num_pipeline = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),  # impute missing values with median\n",
    "                (\"power_transform\", PowerTransformer(standardize=False)),  # additional handling for skewed data\n",
    "                (\"scaler\", StandardScaler())  # scale the data\n",
    "            ])\n",
    "            \n",
    "            cat_pipeline = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # impute missing values with most frequent value\n",
    "                (\"onehotencoder\", OneHotEncoder(handle_unknown='ignore')),  # one hot encode the categorical variables\n",
    "                (\"scaler\", StandardScaler(with_mean=False))  # scale the data (with_mean=False for sparse matrices)\n",
    "            ])\n",
    "            \n",
    "            column_transformer = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num_pipeline\", num_pipeline, self.config.metric_features),\n",
    "                    (\"cat_pipeline\", cat_pipeline, self.config.non_metric_features)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            preprocessing_steps.append((\"column_transformer\", column_transformer))\n",
    "            \n",
    "            # Create the full pipeline\n",
    "            preprocessor = Pipeline(preprocessing_steps)\n",
    "            \n",
    "            return preprocessor\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise (e)\n",
    "    \n",
    "    def initiate_data_transformation(\n",
    "        self, \n",
    "        transforming_data_path: Path, \n",
    "        remove_outliers: bool = True,\n",
    "        save_transformer: bool = True\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This function is responsible for transforming the data.\n",
    "        \n",
    "        Args:\n",
    "            transforming_data_path: Path to the CSV file to transform\n",
    "            numerical_columns: List of numerical column names\n",
    "            categorical_columns: List of categorical column names\n",
    "            remove_outliers: Whether to remove outliers (default: True)\n",
    "            save_transformer: Whether to save the transformer for later use (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # read the data from the specified path\n",
    "            df = pd.read_csv(transforming_data_path)\n",
    "            \n",
    "            # get the data transformer object with outlier removal integrated into the pipeline\n",
    "            data_transformer = self.get_data_transformer_object( remove_outliers)\n",
    "            \n",
    "            # transform the data\n",
    "            transformed_data = data_transformer.fit_transform(df)\n",
    "            \n",
    "            # save the transformed data\n",
    "            self._save_object(transformed_data, self.config.transformed_data_path)\n",
    "            \n",
    "            # save the transformer if requested\n",
    "            if save_transformer:\n",
    "                self._save_object(data_transformer, self.config.transformer_path)\n",
    "            \n",
    "            return transformed_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83af0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 15:54:42,329: INFO: common: YAML file loaded successfully: C:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\config\\config.yaml]\n",
      "[2025-05-14 15:54:42,356: INFO: common: YAML file loaded successfully: C:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\params.yaml]\n",
      "[2025-05-14 15:54:42,361: INFO: common: created directory at: artifacts]\n",
      "[2025-05-14 15:54:42,364: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts\\\\data_ingestion\\\\data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     transformation_instance\u001b[38;5;241m.\u001b[39minitiate_data_transformation(transformation_config\u001b[38;5;241m.\u001b[39mtransforming_data_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     transformation_instance \u001b[38;5;241m=\u001b[39m DataTransformation(config\u001b[38;5;241m=\u001b[39mtransformation_config)\n\u001b[0;32m      5\u001b[0m     transformation_instance\u001b[38;5;241m.\u001b[39mget_data_transformer_object()\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtransformation_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_data_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformation_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforming_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[7], line 194\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, transforming_data_path, remove_outliers, save_transformer)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed_data\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (e)\n",
      "Cell \u001b[1;32mIn[7], line 176\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, transforming_data_path, remove_outliers, save_transformer)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mThis function is responsible for transforming the data.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    Transformed data\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# read the data from the specified path\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransforming_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# get the data transformer object with outlier removal integrated into the pipeline\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     data_transformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_transformer_object( remove_outliers)\n",
      "File \u001b[1;32mc:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\tchok\\OneDrive\\Bureau\\My_github\\clustering-insured-population\\venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts\\\\data_ingestion\\\\data.csv'"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    config = ConfigurationManager()\n",
    "    transformation_config = config.get_transformation_config()\n",
    "    transformation_instance = DataTransformation(config=transformation_config)\n",
    "    transformation_instance.get_data_transformer_object()\n",
    "    transformation_instance.initiate_data_transformation(transformation_config.transforming_data_path)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
